{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the text cleaning and preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "def cleantext(string):\n",
    "    # Remove all punctuation\n",
    "    string = re.sub(r\"'s\\b\", '', string)\n",
    "    string = re.sub(r'[^\\w\\s]', '', string)\n",
    "    # Make all lowercase\n",
    "    string = string.lower()\n",
    "    # remove all stopwords\n",
    "    string = ' '.join([word for word in string.split() if word not in stopwords])\n",
    "    # Remove all special characters\n",
    "    string = re.sub(r'\\W+', ' ', string)\n",
    "    return string\n",
    "def lemmatize(string):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    string = ' '.join([lemmatizer.lemmatize(word) for word in string.split()])\n",
    "    return string\n",
    "def preprocess(obj):\n",
    "    obj = cleantext(obj)\n",
    "    obj = lemmatize(obj)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Datasets/news_data_labelled.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data['Text'] = data['Title'] + ' ' + data['Text']\n",
    "data['cleaned_text'] = data['Text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise the text and convert to sequences of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequences to length of 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pad = pad_sequences(sequences, maxlen=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.get_dummies(data['Sentiment']).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data_pad, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lstm_units=64, dropout_rate=0.2, learning_rate=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1, 100, input_length=400))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning through a coarse parameter grid search, to narrow values down to a smaller range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'lstm_units': [64, 128],\n",
    "    'dropout_rate': [0.2, 0.4],\n",
    "    'learning_rate': [0.01, 0.001]\n",
    "}\n",
    "\n",
    "# This will generate all combinations of parameters\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Placeholder for best score and best params\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "# Iterate over all combinations\n",
    "for params in grid:\n",
    "    # Create a new model with current params\n",
    "    model = create_model(lstm_units=params['lstm_units'],\n",
    "                         dropout_rate=params['dropout_rate'],\n",
    "                         learning_rate=params['learning_rate'])\n",
    "    \n",
    "    # Train the model (use a smaller subset of data for speed)\n",
    "    history = model.fit(X_train[:int(len(X_train)*0.1)], y_train[:int(len(y_train)*0.1)],\n",
    "                        batch_size=32,\n",
    "                        epochs=3,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Check if the performance is improved\n",
    "    score = max(history.history['val_accuracy'])  # Use the best epoch's accuracy\n",
    "    print(f\"Params: {params}, Score: {score}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best params: {best_params}, Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing smaller range to find optimal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lstm_units=64, dropout_rate=0.2, learning_rate=0.01, lstm_layers=1):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(word_index) + 1, output_dim=100, input_length=400))\n",
    "    for i in range(lstm_layers):\n",
    "        return_sequences = i < lstm_layers - 1  # Only the last LSTM layer has return_sequences=False\n",
    "        model.add(LSTM(lstm_units, return_sequences=return_sequences))\n",
    "        if return_sequences:  # Optionally add dropout after each LSTM layer except the last one\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))  # Assuming 3 classes for the output layer\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a new grid with narrowed down parameters and including the number of LSTM layers\n",
    "param_grid = {\n",
    "    'lstm_units': [64],  # Narrowed down to the best value from the previous search\n",
    "    'dropout_rate': [0.1, 0.2, 0.3],  # Slightly broader around the best value\n",
    "    'learning_rate': [0.005, 0.01, 0.02],  # Slightly broader around the best value\n",
    "    'lstm_layers': [1, 2]  # Testing both 1 and 2 layers\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Iterate over the grid\n",
    "for params in grid:\n",
    "    model = create_model(lstm_units=params['lstm_units'],\n",
    "                         dropout_rate=params['dropout_rate'],\n",
    "                         learning_rate=params['learning_rate'],\n",
    "                         lstm_layers=params['lstm_layers'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, \n",
    "                        batch_size=32, \n",
    "                        epochs=3,  # Assuming you want to keep epochs low for quick iterations\n",
    "                        validation_data=(X_val, y_val), \n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Use the best epoch's accuracy as the score\n",
    "    score = max(history.history['val_accuracy'])\n",
    "    print(f\"Params: {params}, Validation Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = create_model(lstm_units=best_params['lstm_units'],\n",
    "                         dropout_rate=best_params['dropout_rate'],\n",
    "                         learning_rate=best_params['learning_rate'])\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=3,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_val, y_val, verbose=0)\n",
    "print('Validation Loss:', score[0])\n",
    "print('Validation Accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return continous output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Preprocess the text\n",
    "    preprocessed_text = preprocess(text)\n",
    "    \n",
    "    # Tokenize and pad the text\n",
    "    sequence = tokenizer.texts_to_sequences([preprocessed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=400)\n",
    "    \n",
    "    # Get the model's prediction (probabilities for each class)\n",
    "    prediction = model.predict(padded_sequence)[0]\n",
    "    \n",
    "    # Assuming the order of output probabilities is [negative, neutral, positive]\n",
    "    # We can take a weighted sum of the probabilities and the sentiment scores\n",
    "    sentiment_score = (prediction[0] * -1) + (prediction[1] * 0) + (prediction[2] * 1)\n",
    "    \n",
    "\n",
    "    return sentiment_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
